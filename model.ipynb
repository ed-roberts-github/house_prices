{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from category_encoders import TargetEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, Lasso, LassoCV, LassoLarsCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "I will begin by using the knowledge I gained in my EDA to remove outliers and drop irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('./house-prices-advanced-regression-techniques/train.csv', index_col=0)\n",
    "\n",
    "df_dropped = df_all.drop(['PoolQC', 'MiscFeature', 'Alley'], axis=1)\n",
    "df = df_dropped[[\n",
    "       'OverallQual', \n",
    "       'Neighborhood', \n",
    "       'GarageArea', \n",
    "       'GrLivArea', \n",
    "       'YearBuilt',\n",
    "       'TotalBsmtSF', \n",
    "       'LotArea', \n",
    "       'BsmtQual', \n",
    "       'ExterQual',\n",
    "       'KitchenQual', \n",
    "       '1stFlrSF', \n",
    "       'MSSubClass', \n",
    "       'YearRemodAdd',\n",
    "       'FullBath',\n",
    "       'GarageFinish', \n",
    "       'GarageYrBlt', \n",
    "       'LotFrontage', \n",
    "       'FireplaceQu',\n",
    "       'TotRmsAbvGrd', \n",
    "       'SalePrice'\n",
    "       ]].copy()\n",
    "df = df.drop(df[(df['SalePrice']<300000) & (df['GrLivArea'] > 4000)].index)\n",
    "\n",
    "\n",
    "y = df.pop('SalePrice')\n",
    "log_y = np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "One could argue there is little need for a pipeline with a relatively simple model, however it is good pratice beacuse it reduces data leakage in cross validation and allows for easy experimentation with models and preprocessing methods.\n",
    "\n",
    "Remember a column transformer can be used to transform different columns but it doesn't link the transforms togther. So if you need to apply to transformations to one column you need to put them in another pipeline then put that pipeline in the column transformer, as I've done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to just use multivariate imputer for this I think\n",
    "# def custom_imputer(df):\n",
    "#     df['LotFrontage'] = df.groupby(by='Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "#     return df\n",
    "# custom_imputer_transformer = FunctionTransformer(func=custom_imputer, validate=False)\n",
    "\n",
    "def log_scaler(df):\n",
    "    df['GrLivArea'] = np.log(df['GrLivArea'])\n",
    "    return df\n",
    "\n",
    "log_transformer = FunctionTransformer(func=log_scaler, validate=False)\n",
    "\n",
    "# Orindal encoding setup\n",
    "ordinal_features = [\n",
    "                    'GarageFinish',\n",
    "                    'BsmtQual',\n",
    "                    'ExterQual',\n",
    "                    'KitchenQual', \n",
    "                    'FireplaceQu'\n",
    "                    ]\n",
    "five_lvls = ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "garage_lvls = ['None', 'Unf', 'RFn', 'Fin']\n",
    "\n",
    "\n",
    "# Target encoding\n",
    "target_enc_features = ['Neighborhood']\n",
    "\n",
    "# Onehot encoding setup\n",
    "onehot_features = ['MSSubClass','Neighborhood']\n",
    "\n",
    "# Features to log scale\n",
    "log_features = ['GrLivArea']\n",
    "\n",
    "\n",
    "preprocessPipe1 = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')),\n",
    "    ('encoder', OrdinalEncoder(categories=[garage_lvls,five_lvls,five_lvls,five_lvls,five_lvls], \n",
    "                                        handle_unknown='error',\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('Pipe1', preprocessPipe1, ordinal_features),\n",
    "        ('MedianImputer', SimpleImputer(strategy='median'), ['LotFrontage']),\n",
    "        ('imputeGaragerYrBlt', SimpleImputer(strategy='constant', fill_value=0), ['GarageYrBlt']),\n",
    "        ('log_scaler', log_transformer, log_features),\n",
    "        # ('targetEncoder', MEstimateEncoder( m=.06), target_enc_features),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'), onehot_features),\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "\n",
    "# Create the final pipeline with the preprocessor and your model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', ElasticNet(alpha=0.0001, l1_ratio=1.0))\n",
    "])\n",
    "\n",
    "# lebosh = pd.DataFrame(preprocessor.fit_transform(df))\n",
    "# # lebosh.isna().any().sum()\n",
    "# lebosh.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSLE in SalePrice is : \n",
      "0.13409685261578358\n"
     ]
    }
   ],
   "source": [
    "def cv(my_pipeline):\n",
    "    scores = -1 * cross_val_score(my_pipeline, df, log_y,\n",
    "                                  cv=5,\n",
    "                                  scoring='neg_mean_squared_error')\n",
    "    return np.mean(np.sqrt(scores))\n",
    "\n",
    "print(f'The RMSLE in SalePrice is : \\n{cv(pipeline)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.427e+00, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.178e+00, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.091e+00, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__alpha': 0.0001, 'model__l1_ratio': 1.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'model__alpha' : [0.01, 0.001, 0.0001, 0.00001],\n",
    "          'model__l1_ratio': [1.0, 0.9, 0.8, 0.7]}\n",
    "\n",
    "gs= GridSearchCV(pipeline,\n",
    "                    param_grid=params,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    cv=5,\n",
    "                    verbose=1\n",
    "                )\n",
    "\n",
    "gs.fit(df, log_y)\n",
    "gs.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
