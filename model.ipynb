{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from category_encoders import TargetEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, Lasso, LassoCV, LassoLarsCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "I will begin by using the knowledge I gained in my EDA to remove outliers and drop irrelevant columns.\n",
    "\n",
    "Once my base models were built, I reintroduced other variables back in. I added them back in from highest Mutual Information/Correlation downwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('./house-prices-advanced-regression-techniques/train.csv', index_col=0)\n",
    "\n",
    "# Feature engineering: total bathrooms\n",
    "df_all['TotalBath'] = df_all['BsmtFullBath'] + df_all['FullBath'] + 0.5*df_all['BsmtHalfBath'] + 0.5*df_all['HalfBath']\n",
    "\n",
    "df_dropped = df_all.drop(['PoolQC', 'MiscFeature', 'Alley'], axis=1)\n",
    "df = df_dropped[[\n",
    "       'OverallQual', \n",
    "       'Neighborhood', \n",
    "       'GarageArea', \n",
    "       'GrLivArea', \n",
    "       'YearBuilt',\n",
    "       'TotalBsmtSF', \n",
    "       'LotArea', \n",
    "       'BsmtQual', \n",
    "       'ExterQual',\n",
    "       'KitchenQual', \n",
    "       '1stFlrSF', \n",
    "       'MSSubClass', \n",
    "       'YearRemodAdd',\n",
    "       'FullBath',\n",
    "       'GarageFinish', \n",
    "       'GarageYrBlt', \n",
    "       'LotFrontage', \n",
    "       'FireplaceQu',\n",
    "       'TotRmsAbvGrd', \n",
    "\n",
    "       'TotalBath',\n",
    "       'Foundation',\n",
    "       'GarageType',\n",
    "       'OpenPorchSF',\n",
    "       'HeatingQC',\n",
    "       'Fireplaces',\n",
    "       'MSZoning',\n",
    "       'OverallCond',\n",
    "\n",
    "       'SalePrice'\n",
    "       ]].copy()\n",
    "df = df.drop(df[(df['SalePrice']<300000) & (df['GrLivArea'] > 4000)].index)\n",
    "\n",
    "\n",
    "y = df.pop('SalePrice')\n",
    "log_y = np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "One could argue there is little need for a pipeline with a relatively simple model, however it is good pratice beacuse it reduces data leakage in cross validation and allows for easy experimentation with models and preprocessing methods.\n",
    "\n",
    "Remember a column transformer can be used to transform different columns but it doesn't link the transforms togther. So if you need to apply to transformations to one column you need to put them in another pipeline then put that pipeline in the column transformer, as I've done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to just use multivariate imputer for this I think\n",
    "# def custom_imputer(df):\n",
    "#     df['LotFrontage'] = df.groupby(by='Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "#     return df\n",
    "# custom_imputer_transformer = FunctionTransformer(func=custom_imputer, validate=False)\n",
    "\n",
    "def log_scaler(df):\n",
    "    df['GrLivArea'] = np.log(df['GrLivArea'])\n",
    "    return df\n",
    "\n",
    "log_transformer = FunctionTransformer(func=log_scaler, validate=False)\n",
    "\n",
    "# Orindal encoding setup\n",
    "ordinal_features = [\n",
    "                    'GarageFinish',\n",
    "                    'BsmtQual',\n",
    "                    'ExterQual',\n",
    "                    'KitchenQual', \n",
    "                    'FireplaceQu',\n",
    "                    'HeatingQC'\n",
    "                    ]\n",
    "five_lvls = ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "garage_lvls = ['None', 'Unf', 'RFn', 'Fin']\n",
    "all_lvls = [garage_lvls,five_lvls,five_lvls,five_lvls,five_lvls,five_lvls]\n",
    "\n",
    "\n",
    "# Target encoding\n",
    "target_enc_features = ['Neighborhood']\n",
    "\n",
    "# Onehot encoding setup\n",
    "onehot_features = ['MSSubClass','Neighborhood','Foundation','MSZoning']\n",
    "\n",
    "# Features to log scale\n",
    "log_features = ['GrLivArea']\n",
    "\n",
    "# Features that are missing and need onehot enc\n",
    "missing_onehot = ['GarageType']\n",
    "\n",
    "features_impute_zero = ['GarageYrBlt','OpenPorchSF','Fireplaces']\n",
    "\n",
    "preprocessPipe1 = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')),\n",
    "    ('encoder', OrdinalEncoder(categories=all_lvls, \n",
    "                                        handle_unknown='error',\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocessPipe2 = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')),\n",
    "    ('OH', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('Pipe1', preprocessPipe1, ordinal_features),\n",
    "        ('Pipe2', preprocessPipe2, missing_onehot),\n",
    "        ('MedianImputer', SimpleImputer(strategy='median'), ['LotFrontage']),\n",
    "        ('imputeZero',  SimpleImputer(strategy='constant', fill_value=0), features_impute_zero),\n",
    "        ('log_scaler', log_transformer, log_features),\n",
    "        ('imputeCond',SimpleImputer(strategy='constant', fill_value=5),['OverallCond']),\n",
    "        # ('targetEncoder', MEstimateEncoder( m=.06), target_enc_features),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'), onehot_features),\n",
    "        \n",
    "    ], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking everything is working with a quick cheeky Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSLE in SalePrice is : \n",
      "0.13901718011759137\n"
     ]
    }
   ],
   "source": [
    "# Create the final pipeline with the preprocessor and your model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(n_estimators=230, random_state=1))\n",
    "])\n",
    "\n",
    "def cv(my_pipeline):\n",
    "    scores = -1 * cross_val_score(my_pipeline, df, log_y,\n",
    "                                  cv=5,\n",
    "                                  scoring='neg_mean_squared_error')\n",
    "    return np.mean(np.sqrt(scores))\n",
    "\n",
    "print(f'The RMSLE in SalePrice is : \\n{cv(pipeline)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "\n",
    "First, I will now look at a few linear regression models (including regularised models such as Ridge and Lasso), running a grid search on their parameters.\n",
    "\n",
    "I will then go onto train ensemble models, such as Random Forest, XGBoost.\n",
    "\n",
    "I first looked at implementing the hyperparameterisation with Optuna, but decided this was unnecessariily complex and made my code unreadable. I have gone back to using simple GridSearchCV\n",
    "\n",
    "There may be a way to integrate this individual model search\n",
    "    into GridSearchCV however because each modle has a different param\n",
    "    set I proceeded this way to make it simpler\n",
    "\n",
    "### Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_search(model, params, tracker):\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    gs= GridSearchCV(pipeline,\n",
    "                    param_grid=params,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    cv=5,\n",
    "                    verbose=1\n",
    "                )\n",
    "    \n",
    "    gs.fit(df,log_y);\n",
    "\n",
    "    scores = np.array([-1*gs.cv_results_[f'split{i}_test_score'] for i in range(gs.n_splits_)])\n",
    "    best_RMSLE = min(np.mean(np.sqrt(scores),axis=1))\n",
    "\n",
    "    # mean_score = -1 * gs.cv_results_['mean_test_score']\n",
    "    # print(mean_score)\n",
    "    # best_RMSLE = np.sqrt(max(mean_score))\n",
    "    \n",
    "    tracker[str(model)] = best_RMSLE\n",
    "    \n",
    "    print(f'For {str(model)}, optimal param when {gs.best_params_} with score RMSLE: \\n{best_RMSLE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    scores_sd = cv_results['std_test_score']\n",
    "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 400 candidates, totalling 2000 fits\n",
      "For Ridge(alpha=0.0001), optimal param when {'model__alpha': 2.6909999999999994} with score RMSLE: \n",
      "0.1096848860456533\n"
     ]
    }
   ],
   "source": [
    "tracker = {}\n",
    "\n",
    "model = Ridge(alpha=0.0001)\n",
    "params = {'model__alpha' : list(np.arange(0.001,4,0.01))}\n",
    "run_search(model,params,tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.975e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.559e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.934e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.368e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.751e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.127e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.491e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.846e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.190e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.437e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.752e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/edroberts/opt/anaconda3/envs/trading/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.059e-02, tolerance: 1.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Lasso(alpha=0.0001), optimal param when {'model__alpha': 0.00023} with score RMSLE: \n",
      "0.11101050369685109\n"
     ]
    }
   ],
   "source": [
    "model = Lasso(alpha=0.0001)\n",
    "params = {'model__alpha' : list(np.arange(0.0001,0.001,0.00001))}\n",
    "run_search(model,params,tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "For ElasticNet(alpha=0.0001, l1_ratio=1.0), optimal param when {'model__alpha': 0.001, 'model__l1_ratio': 0.7} with score RMSLE: \n",
      "0.22160652468702297\n"
     ]
    }
   ],
   "source": [
    "model = ElasticNet(alpha=0.0001, l1_ratio=1.0)\n",
    "params = {'model__alpha' : list(np.arange(0.001,4,0.01)),\n",
    "          'model__l1_ratio': [1.0, 0.9, 0.8, 0.7]}\n",
    "run_search(model,params,tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ridge(alpha=0.0001)': 0.1096848860456533,\n",
       " 'Lasso(alpha=0.0001)': 0.11101050369685109,\n",
       " 'ElasticNet(alpha=0.0001, l1_ratio=1.0)': 0.22160652468702297}"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "For RandomForestRegressor(), optimal param when {'model__n_estimators': 220, 'model__random_state': 1} with score RMSLE: \n",
      "0.13445663332304122\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "params = {'model__n_estimators': range(30,300,10),\n",
    "          'model__random_state': [1]}\n",
    "run_search(model,params,tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "For XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=1000, n_jobs=4,\n",
      "             num_parallel_tree=None, random_state=None, ...), optimal param when {'model__learning_rate': 0.03, 'model__n_estimators': 500} with score RMSLE: \n",
      "0.12562884561241103\n"
     ]
    }
   ],
   "source": [
    "model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "params = {'model__n_estimators': range(500,2000,500),\n",
    "          'model__learning_rate' : list(np.arange(0.01,0.1,0.01))\n",
    "          }\n",
    "run_search(model,params,tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ridge(alpha=0.0001)': 0.12484422796655499,\n",
       " 'Lasso(alpha=0.0001)': 0.12613119278150228,\n",
       " 'ElasticNet(alpha=0.0001, l1_ratio=1.0)': 0.12590691889105393,\n",
       " 'RandomForestRegressor()': 0.13609999363981382,\n",
       " 'XGBRegressor(base_score=None, booster=None, callbacks=None,\\n             colsample_bylevel=None, colsample_bynode=None,\\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\\n             enable_categorical=False, eval_metric=None, feature_types=None,\\n             gamma=None, grow_policy=None, importance_type=None,\\n             interaction_constraints=None, learning_rate=0.05, max_bin=None,\\n             max_cat_threshold=None, max_cat_to_onehot=None,\\n             max_delta_step=None, max_depth=None, max_leaves=None,\\n             min_child_weight=None, missing=nan, monotone_constraints=None,\\n             multi_strategy=None, n_estimators=1000, n_jobs=4,\\n             num_parallel_tree=None, random_state=None, ...)': 0.1324534526687697}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
